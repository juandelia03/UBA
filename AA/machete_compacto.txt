Medidas de homogeneidad(M(S) es la homogeneidad):
- Classification Error Rate: $1 - max_{k \in K}(p(k))$
- Impureza gini: $1 - \sum_{k \in clases}{p(k)^2}$
- Entropía: $H(S) = - \sum_{k \in clases(S)}{p(k) log_2 p(k)}$
$\Delta M(S,<a,c>)$ la ganancia de realizar un corte$ puede ser Info gain (M = entropía), Ganancia Gini (M = impureza gini) o CER
$\Delta M(S, <a,c>) = M(S) - (prop_{\leq} \cdot M(S_{\leq}) + prop_{>} \cdot M(S_{>}))$
sesgo árboles:- El tipo de regiones de decisión que puede generar tiene forma de (hiper-)rectángulos.
- Las regiones que exploramos se determinan de manera Greedy.
- Atributos más discriminativos -> cerca de la raíz.
- Árboles más pequeños y menos complejos en términos de su estructura de acuerdo al criterio de parada
- De acuerdo a cómo se recorren los distintos cortes para un atributo dado, se pueden perder soluciones
El score asignado a cada clase en una predicción se calcula como la fracción de instancias de cada clase en
la hoja donde cayó la predicción.
El valor de una hoja en regrs es el promedio de las etiquetas de la región.
Importancia: Cada vez que se use un atributo para realizar un corte sumarle a ese atributo la
ganancia (gini/info) de ese corte ponderado por cuantas instancias tenía el nodo.
Con Gini->gini importance,entropia -> Mean decrease impurity.
copiar teorema de bayes
copiar la expresión del clasificador óptimo de bayes y como se deriva x teo de bayes
que proba estiman los discriminativos para predecir (diapo 5)
que proba estiman los generativos para predecir (diapo 21)
En knn y svm ojo hay que normalizar atributos. kernels: lineal, polinómico, gaussiano/radial, Sigmoideo
sesgo nb: 1.asumir que los atributos por clase son independientes 2.asume que las clases se distribuyen de manera normal. Una media por clase, por atributo y un desvio por
atributo
sesgo LDA/QDA: las clases se distribuyen de manera normal. una media por clase, misma matriz de cov para todas en LDA una para cada clase en QDA. LDA frontera lineal QDA parábola.
sesgo knn: La distancia importa. Aprende de de lo que tiene cerca (localmente)
sesgo svm (usa kernel trick para no computar trasnf): Elige separar el espacio usando hiperplanos y maximizando el margen.
- Accuracy: $\frac{\text{predicciones correctas}}{\text{predicciones totales}}$
- Presicion: De las instancias predichas como positivas ¿qué porcentaje lo eran?
- Recall=TPR: De las instancias positivas, ¿qué porcentaje fueron predichas como tal?
- FPR
- $F_\beta$ (F_1 iguall importancia a Prec que Rec)
Curvas ROC (anotar que va en cada eje): Un umbral por cada instancia. Una matriz por umbral. Por cada matriz calculo (TPR,FPR). 
Cada unova a ser un punto de la curva roc
**Matriz de confusión**
- MSE
- MAE
- RMSE
- $R^2$ (que tanto mejor ajusta que la media, no es sensible a escala. se pierde la escala en el score)
- r (pearson) AL MARGEN
Como estamos modelando una función no determinista tenemos un error irreducible.
Fórmulas de sesgo y
varianza y sus definiciones en palabras.
overfit -> varianza alta
underfit -> sesgo alto
reducir(flechita para abajo) var:Seleccionar modelos simples, Reducción dimensional, Regularización (pruning), ensambles (bagging)
reducir sesgo: modelos más complejos, Extraer más features, ensambles (boosting)
Bootrap: Dado un dataset, crear otros (B) del mismo tamaño con instancias (filas) elegidas al azar (con reposición)
Bagging: -Hacer muchos training sets distintos del mismo tomando instancias al azar del dataset original (con reposición)
- construir un modelo por cada set
- promediar las predicciones para hacer la predicción final
Problema: los árboles están muy correlacionados solución-> RF. 
Para reducir la varianza no basta con agregar modelos, hay que disminuir su correlación.
(si queda espacio meter la cuenta)
Random forest = Bootstrap + se miran m atributos random en cada split.
Out-of-bag: Obtener estimaciones de qué tan bien generalizan los modelos.
Para cada instancia, utilizar los árboles que no contienen a x (como ensamble) en su conjunto de entrenamiento para predecir la clase de x . 
Permite probar al modelo a medida que se entrena. Menos cómputo (en comparación a CV).
Boosting: combinar clasificadores con alto sesgo de manera que cada clasificador se convierta en un “experto” en los errores que cometen los clasificadores anteriores.
definición sesgo:..
sigue.